# Proposal for standalone Azure cloud provider

## Overview

### Background
As mentioned in doc [Refactor Cloud Provider out of Kubernetes Core](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cloud-provider-refactoring.md), all cloud provider specific code is planned to be removed in Release V1.9.

This article discusses the action plan for migrating current Azure cloud provider code to a new repository.

### Changes for source code
Cloud provider refactoring has two aspects:
1. Run cloud provider related code in a separate process (move out from controller-manager)
2. Move cloud provider related code in a separate repository (move out from kubernetes repository)

For #1, there is already the [cloud controller manager](https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager), which could be used to run cloud provider logic outside of controller manager. And when moving to cloud provider's own repository, we could build an application upon the [cloud controller manager](https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager) library. Examples: [rancher-cloud-controller-manager](https://github.com/rancher/rancher-cloud-controller-manager) , [keepalived-cloud-provider](https://github.com/munnerz/keepalived-cloud-provider).

For #2, this means we should firstly create a new repository (e.g. https://github.com/Azure/kubernetes-azure-cloud-provider), and then move existing cloud provider related code into it.
Things to consider includes ongoing issues/pull requests, source code licenses headers. Will discuss more below.

Another precondition for moving code out is to cut off all direct dependencies (on Azure cloud provider) in kubernetes repo.

Currently components depending Azure cloud provider includes: [azuer_credentials](https://github.com/kubernetes/kubernetes/blob/master/pkg/credentialprovider/azure/azure_credentials.go), [AzureDataDisk](https://github.com/kubernetes/kubernetes/tree/master/pkg/volume/azure_dd), [AzureFile](https://github.com/kubernetes/kubernetes/tree/master/pkg/volume/azure_file), and [e2e framework](https://github.com/kubernetes/kubernetes/blob/master/test/e2e/framework/BUILD#L60). Will discuss in detail below.

### Changes for runtime
Three runtime binaries are involved: `kubelet`, `apiserver`, `controller-manager`. Currently all of them have dependent logic on cloud provider, and thus accepting a parameter '--cloud-provider'.

The challenge here is to segergate cloud provider related logic from all the 3 biniaries. For `controller-manager` we will be using `cloud-controller-manager`. For `kubelet` and `apiserver`, part of the logic could be moved to `cloud-controller-manager`, and the remaning part needs other approaches.

Here is to highlight the parameters that need to be update for runtime.

1. With built-in cloud providers (current behaviour):

Master node:
```
kubelet                     --cloud-provider={provider_name} --cloud-config={config_path}
apiserver                   --cloud-provider={provider_name} --cloud-config={config_path}
controller-manager          --cloud-provider={provider_name} --cloud-config={config_path}
```

Agent node:
```
kubelet                     --cloud-provider={provider_name} --cloud-config={config_path}
```

2. With independent cloud-controller-manager:

Master node:
```
kubelet                     --cloud-provider=external
apiserver                   --cloud-provider=external
controller-manager          --cloud-provider=external
cloud-controller-manager    --cloud-provider={provider_name} --cloud-config={config_path}

```

Agent node:
```
kubelet                     --cloud-provider=external
```

## Tasks
Here is a list of tasks in chronological order (with suggested time slot). The task title should be quite self-explanatory. More details could be found in following section.

- [ ] Stage 1 - Preparing (v1.8 release)
    - [ ] Test run and test coverage for cloud provider related E2E cases
    - [ ] AzureDisk&AzureFile support via FlexVolume
    - [ ] AzureDisk&AzureFile dynamic provision support
    - [ ] Azure Credential Provider support via Azure SDK
- [ ] Stage 2 - Transition between repository (v1.8 release)
    - [ ] Tool for migrating pull request and issues
    - [ ] Tool for syncing code from kuberntes repository to new repository
    - [ ] Sync code and run tests continusly from upstream
- [ ] Stage 3 - Move to new code repository (v1.9 release)
    - [ ] Mark all PR with cloud provider code change as `do-not-merge`, add instructions for porting cloud provider specific code to new PR
    - [ ] Ensure E2E test result
    - [ ] Send PR to delete cloud provider related code in Kubernetes repo

## Details - Code Logic Change

### Volume
Volume support involves two components:
`controller-manager`    : Disk attach/provision via cloud provider (when [EnableControllerAttachDetach](https://github.com/kubernetes/kubernetes/pull/26351) is on)
`kubelet`               : Disk mounting on agent nodes

And we have got 2 volume plugins depending on Azure cloud provider: 'Azure Disk' and 'Azure File'. Both support dynamic provision.

#### Dependencies
- azure_dd
1. attacher.go      : Calls cloudprovider to get instance id by nodename
2. attacher.go      : Calls cloudprovider to do attach/deattach data disk
3. azure_common.go  : Calls MakeCRC32 in cloudprovider's util func

- azure_file
1. azure_file.go        : Calls cloudprovider to get cloud environment dependent endpoint
2. azure_provision.go   : Calls cloudprovider to create and delete Azure File instance

#### Issues
1. Attach/provision not supported by cloud-controller-manager

    The cloud-controller-manager already supports 'NodeController', 'RouteController', 'ServiceController', but not 'PersistentVolumeBinderController' and 'AttachDetachController' So 'dynamic volume creation' and 'attaching volume to node' are not supported by cloud-controller-manager.

    As mentioned in [Refactor Cloud Provider out of Kubernetes Core](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cloud-provider-refactoring.md), a potential solution is using [FlexVolume](https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md), which supports external volume drivers.

    We need to first verify whehter FlexVolume can well work with 'Azure Disk' and 'Azure File'.

    Note that using FlexVolume directly would require users to update their `AzureFileVolume` and `AzureDiskVolume` to `FlexVolume`. And so do other cloud provider supported volumes.

    One solution is to auto fallback to `FlexVolume` when `AzureFileVolume` or `AzureDiskVolume` is specified, but not cloud provider is set.

    Also FlexVolume does not seem to support dynamic provision now. This could be another pending issue.

2. Kubelet still depends on the volume plugin which relies on Cloud Provider

    Kubelet just wants the mounting logic, which is cloud provider independent. But the whole volume plugin has dependency on cloud provider.

    Pending issue, should be solved together with#1

#### Tasks
`AzureDisk&AzureFile support via FlexVolume`, `AzureDisk&AzureFile dynamic provision support`.

### E2E test

#### Dependencies
- test/e2e/framework/service_util.go : Calls cloud provider to get annotation literal, used by internal loadbalancer annotation test.
    Related PRs: https://github.com/kubernetes/kubernetes/pull/45473, https://github.com/kubernetes/kubernetes/pull/49341

- test/e2e/e2e.go, test/e2e/framework/pv_util.go:GetAzureCloud, test/e2e/framework/util.go::GetAzureCloud: data disk e2e: For seting up azure data disk for disk E2E. test. Related PR: https://github.com/kubernetes/kubernetes/pull/45109

#### Issues

1. E2E test migration

    The overall E2E test migration plan may be pending on e2e tests for [cloud-controller-manager](https://github.com/kubernetes/kubernetes/issues/44975)

    There are several kinds of E2E cases related to cloud provider:

    1. The test could be run against 'azure' cloud, but do not depend on Azure cloud provider:
    Those include all conformace tests and some cloud provider specific tests (like azure internal loadbalancer).

    2. The test depend on Azure cloud provider directly:
    Currently serverl tests introduced by azure disk dynamic provision.

    For #2 we could get rid of the dependency by  rewriting it with Azure SDK.

2. E2E test run

    For running E2E tests we need to setup a cluster first. Things become a bit tricky since we'll no longer have cloud specific code in kubernets build output. That means the docker image for kuberentes binaries and for cloud provider binaries will be coming from two sources. Will need to figure a way to get E2E test run well in this case.

3. E2E test passage

    Before switching to new repository, we need to esure test result is identical between running 'controller-manager built-in cloud provdier' and 'independent cloud-controller-manager'.


`Test run and test coverage for cloud provider related E2E cases`, `Ensure E2E test result`.


### Credential Provider
#### Dependencies
azure_credentials.go: Calls cloud provider to get credentials for image pulling from ACR

#### Issues
As per disscussion in [Issue#48690](https://github.com/kubernetes/kubernetes/issues/48690), an out-of-tree credential provider is proposed.

For current code in azure_credentials, it only use cloud provider to parse configuration and get auth token.

So one solution is to rewrite related logic with Azure SDK directly.

#### Tasks
`Azure Credential Provider support via Azure SDK`

## Details - Non Code Logic Change

### Ongoing Issues and Pull Request
When we do the code change, there may be ongoing issues and pull request addressing the cloud provider code.

Before switching to the new repository, the issues can be left intact, and PR still goes to kubernetes repo.

After switching to the new repository, we could advise users to turn to new repository to create new issues, and can also provide a tool (like the [cherry pick tool](https://github.com/kubernetes/kubernetes/blob/master/hack/cherry_pick_pull.sh) ) for easier migrating existing issues/PR changes to the new repository.

Related tasks: `Tool for migrating pull request and issues`

### Code Migration tool
To get start with the new repository, we may first need to import the cloud provider related code, and then implement a entry point to start the cloud controller manager.

The importing logic could be done via a automation sync tool with some simple mapping rule.

Also during Stage 2, we will be verifing against new cloud controller manager built from new repository, while we can still allow user check in code to old repository. In this case, we can use the sync tool to pulling code from kubernetes repo to verify the change continously.

Related tasks: `Tool for syncing code from kuberntes repository to new repository`, `Sync code and run tests continusly from upstream`

### Code License Header
All cloud provier source code have a license header. We should check and see whether the header needs some update during transition.

If required, the automation sync tool can handle the change.

Related task: `Tool for syncing code from kuberntes repository to new repository`



