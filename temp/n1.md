# Proposal for standalone Azure cloud provider

## Overview

### Background
As mentioned in doc [Refactor Cloud Provider out of Kubernetes Core](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cloud-provider-refactoring.md), all cloud provider specific code is planned to be removed in Release V1.9.

This article discusses the action plan for migrating current Azure cloud provider code to a new repository.

### Changes for source code
Cloud provider refactoring has two aspects:
1. Run cloud provider related code in a separate process (move out from controller-manager)
2. Move cloud provider related code in a separate repository (move out from kubernetes repository)

For #1, there is already the [cloud controller manager](https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager), which could be used to run cloud provider logic outside of controller manager. And when moving to cloud provider's own repository, we could build an application upon the [cloud controller manager](https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager) library. Examples: [rancher-cloud-controller-manager](https://github.com/rancher/rancher-cloud-controller-manager) , [keepalived-cloud-provider](https://github.com/munnerz/keepalived-cloud-provider).

For #2, this means we should firstly create a new repository (e.g. https://github.com/Azure/kubernetes-azure-cloud-provider), and then move existing cloud provider related code into it.
Things to consider includes ongoing issues/pull requests, source code licenses headers. Will discuss more below.

Another precondition for moving code out is to cut off all direct dependencies (on Azure cloud provider) in kubernetes repo.

Currently components depending Azure cloud provider includes: [azuer_credentials](https://github.com/kubernetes/kubernetes/blob/master/pkg/credentialprovider/azure/azure_credentials.go), [AzureDataDisk](https://github.com/kubernetes/kubernetes/tree/master/pkg/volume/azure_dd), [AzureFile](https://github.com/kubernetes/kubernetes/tree/master/pkg/volume/azure_file), and [e2e framework](https://github.com/kubernetes/kubernetes/blob/master/test/e2e/framework/BUILD#L60). Will discuss in detail below.


### Changes for runtime
Here is to highlight the parameters that need to be update for runtime.

With built-in cloud providers (current behaviour):

Master node:
```
kubelet                     --cloud-provider={provider_name} --cloud-config={config_path}
apiserver                   --cloud-provider={provider_name} --cloud-config={config_path}
controller-manager          --cloud-provider={provider_name} --cloud-config={config_path}
```

Agent node:
```
kubelet                     --cloud-provider={provider_name} --cloud-config={config_path}
```

With independent cloud-controller-manager:

Master node:
```
kubelet                     --cloud-provider=external
apiserver                   --cloud-provider=external
controller-manager          --cloud-provider=external
cloud-controller-manager    --cloud-provider={provider_name} --cloud-config={config_path}

```

Agent node:
```
kubelet                     --cloud-provider=external
```

## Tasks
Here is a list of tasks in chronological order (with suggested time slot). The task title should be quite self-explanatory. More details could be found in following section.

- [ ] Stage 1 - Preparing (v1.8 release)
    - [ ] Test run and test coverage for cloud provider related E2E cases
    - [ ] AzureDisk&AzureFile support via FlexVolume
    - [ ] AzureDisk&AzureFile dynamic provision support
    - [ ] Azure Credential Provider support via Azure SDK
- [ ] Stage 2 - Transition between repository (v1.8 release)
    - [ ] Tool for migrating pull request and issues
    - [ ] Tool for syncing code from kuberntes repository to new repository
    - [ ] Sync code and run tests continusly from upstream
- [ ] Stage 3 - Move to new code repository (v1.9 release)
    - [ ] Mark all PR with cloud provider code change as `do-not-merge`, add instructions for porting cloud provider specific code to new PR
    - [ ] Ensure E2E test result
    - [ ] Send PR to delete cloud provider related code in Kubernetes repo

## Details

### Volume
Currently there're 'Azure Disk' and 'Azure File' volume plugins depending on Azure cloud provider.

The cloud-controller-manager already supports 'NodeController', 'RouteController', 'ServiceController', but not 'PersistentVolumeBinderController' and 'AttachDetachController' So 'dynamic volume creation' and 'attaching volume to node' are not supported by cloud-controller-manager.

As mentioned in [Refactor Cloud Provider out of Kubernetes Core](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cloud-provider-refactoring.md), a potential solution is using [FlexVolume](https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md), which supports external volume drivers.

We need to first verify whehter FlexVolume can well work with 'Azure Disk' and 'Azure File'.

Also FlexVolume does not seem to support dynamic provision now. This could be a pending issue.

Related tasks: `AzureDisk&AzureFile support via FlexVolume`, `AzureDisk&AzureFile dynamic provision support`.

### Credential Provider
Currently we also have [azure_credentials](https://github.com/kubernetes/kubernetes/blob/master/pkg/credentialprovider/azure/azure_credentials.go) depending on Azure cloud provider.

As per disscussion [Issue#48690](https://github.com/kubernetes/kubernetes/issues/48690),  a out-of-tree credential provider is one potential solution.

For current code in azure_credentials, it only use cloud provider to parse configuration and get auth token.

So one solution is to use Azure SDK directly to get rid of cloud provider dependency.

Related task: `Azure Credential Provider support via Azure SDK`

### E2E test
The overall E2E test migration plan may be pending on e2e tests for [cloud-controller-manager](https://github.com/kubernetes/kubernetes/issues/44975)

For running E2E tests we need to setup a cluster first. Things become a bit tricky since we'll no longer have cloud specific code in kubernets build output. That means the docker image for kuberentes binaries and for cloud provider binaries will be coming from two sources. Will need to figure a way to get E2E test run well in this case.

There are several kinds of E2E cases related to cloud provider:

1. The test could be run against 'azure' cloud, but do not depend on Azure cloud provider:
Those include all conformace tests and some cloud provider specific tests (like azure internal loadbalancer).

2. The test depend on Azure cloud provider directly:
Currently serverl tests introduced by azure disk dynamic provision.

For #1 we could leave it in upstream repo, and for #2 we could also try rewriting it with Azure SDK.

Finally, before switching to new repository, we need to esure test result is identical between running 'controller-manager built-in cloud provdier' and 'independent cloud-controller-manager'.

Related tasks: `Test run and test coverage for cloud provider related E2E cases`, `Ensure E2E test result`.

### Ongoing Issues and Pull Request
When we do the code change, there may be ongoing issues and pull request addressing the cloud provider code.

Before switching to the new repository, the issues can be left intact, and PR still goes to kubernetes repo.

After switching to the new repository, we could advise users to turn to new repository to create new issues, and can also provide a tool (like the [cherry pick tool](https://github.com/kubernetes/kubernetes/blob/master/hack/cherry_pick_pull.sh) ) for easier migrating existing issues/PR changes to the new repository.

Related tasks: `Tool for migrating pull request and issues`

### Code Migration tool
To get start with the new repository, we may first need to import the cloud provider related code, and then implement a entry point to start the cloud controller manager.

The importing logic could be done via a automation sync tool with some simple mapping rule.

Also during Stage 2, we will be verifing against new cloud controller manager built from new repository, while we can still allow user check in code to old repository. In this case, we can use the sync tool to pulling code from kubernetes repo to verify the change continously.

Related tasks: `Tool for syncing code from kuberntes repository to new repository`, `Sync code and run tests continusly from upstream`

### Code License Header
All cloud provier source code have a license header. We should check and see whether the header needs some update during transition.

If required, the automation sync tool can handle the change.

Related task: `Tool for syncing code from kuberntes repository to new repository`



